{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hello everyone o//</p> <p>The goal of this guide is to teach you how to prompt better.  Better prompts generate lower failure outputs and allow the model to handle more complex prompts better. A bad prompt might still result in a good image every hundred images, but a good prompt will do so more consistently, allowing you to discard less images. </p> <p>If you are just trying the technology out, you can just prompt yourself or copy any random prompt and get okay enough outputs. Refer to Prompting for Dummies.</p> <p>This guide is for people who: 1. Want a basic idea of prompting anime models (base:  Stable Diffusion 1.5 / Novel AI) 2. Want to have high quality pictures more often 3. Need to generate something complex and don't know how 4. Want to keep power consumption low by generating fewer pictures 5. Are interested in prompt engineering</p> A fair warning at the start:  This guide applies mostly to anime models. Photorealistic models do not work the same way. Read a photorealistic guide instead, if there is one. If not: Some concepts do carry over, so don't worry. Refer to the general information list.  Also: The guide is based on what I read and what I experimented with. I do not guarantee the validity of the guide. Please don't trust the guide, run your own experiments.   Note: This guide is SFW.  I want the most people to benefit from this guide. Jumping from SFW to NSFW prompts is easier than from NSFW to SFW, and has the added benefit that you can read this hopefully without triggering some sensors at your workplace.  <p>First off, we want to start with installing the Automatic1111 WebUI .  Then, we want to make sure we have the right Setup. Even experienced users should take a quick look at this section, I have a list at the start.  Then we can already get started with Basic Prompting</p>"},{"location":"Intermediate%20Guide/","title":"Intermediate Guide","text":"<p>Coming to you SoonTM.</p> <p>BREAK introduction There are two scenarios where fewer tokens get you worse pictures.  The first one is obviously when you want to include a hundred different things, and are fine with leaving out some.  The second scenario is complicated tags. If the tags themselves contain too much information or are too much to paint, separating them into different batches is the way to go.</p>"},{"location":"Overview/","title":"Overview","text":"<p>Hello everyone o//</p> <p>The goal of this guide is to teach you how to prompt better.  Better prompts generate lower failure outputs and allow the model to handle more complex prompts better. A bad prompt might still result in a good image every hundred images, but a good prompt will do so more consistently, allowing you to discard less images. </p> <p>If you are just trying the technology out, you can just prompt yourself or copy any random prompt and get okay enough outputs. Refer to Prompting for Dummies.</p> <p>This guide is for people who: 1. Want a basic idea of prompting anime models (base:  Stable Diffusion 1.5 / Novel AI) 2. Want to have high quality pictures more often 3. Need to generate something complex and don't know how 4. Want to keep power consumption low by generating fewer pictures 5. Are interested in prompt engineering</p> <p>Warning:</p> <p>This guide applies mostly to anime models. Photorealistic models do not work the same way. Read a photorealistic guide instead, if there is one. If not: Some concepts do carry over, so don't worry. Refer to the general information list.  Also: The guide is based on what I read and what I experimented with. I do not guarantee the validity of the guide. Please don't trust the guide, run your own experiments. </p> Note: This guide is SFW. <p>I want the most people to benefit from this guide. Jumping from SFW to NSFW prompts is easier than from NSFW to SFW, and has the added benefit that you can read this hopefully without triggering some sensors at your workplace.</p> <p>First off, we want to start with installing the Automatic1111 WebUI .  Then, we want to make sure we have the right Setup. Even experienced users should take a quick look at this section, I have a list at the start.  Then we can already get started with Basic Prompting</p>"},{"location":"1.%20Installation/1.%20Installing%20Automatic1111%20WebUI/","title":"1. Installing Automatic1111 WebUI","text":"Info <p><p>My guide is not an installation guide. Instead, follow this guide.</p></p> <p>As for the model, I recommend the MeinaMix model. For other anime models, I recommend PastelMix, CetusMix, and AbyssOrangeMix3. There are other models, feel free to browse the associated website for them, but I will not detail them in this section. Instead, you can look in Model List.</p> <p>Be aware that for other models you might need to download a so-called \"VAE\" as well. If you do not have it, your pictures will look washed out. </p> How do you add a VAE? <p>Check out [[How to select a VAE|here]]</p> <p>To confirm everything is working, generate a picture, with \"1girl, masterpiece\" as the prompt. Feel free to then play around with the model. If you encounter any errors, re-read the guide, or hit one of the community servers. </p> <p>If you just want to get started ASAP, read: Prompting for Dummies</p> <p>Next: 2. Setup. Read even if experienced. </p>"},{"location":"1.%20Installation/2.%20Setup/","title":"2. Setup","text":"<p>List: General:     1.  Save Prompt and Generation Information To File     2. Add VAE to menu selection</p> <p>Anime models:     3. Set CLIP SKIP to 2     4. Untick Interrogate Deepbooru sort alphabetically (maybe it's that way by default idk)</p> <p>Extra:     5. Tick K-Diffusion Samplers produce same image in a batch (important for xyz matrix)</p> <p>Prev: 1. Installing Automatic1111 WebUI Next: 1. Fundamentals of Image Generation Models</p>"},{"location":"1.%20Installation/2.1%20Save%20Prompt%20and%20Generation%20Information%20To%20File/","title":"2.1 Save Prompt and Generation Information To File","text":"<p>This makes it so that any picture you generate has all the details to re-generate that picture. I don't know if it's on by default, so you better check it.\u00a0  Scroll down.  The lower of the two marked boxes is the important one. The top one is also a nice one to have.</p> <p> Here's an example of this in action: <p> </p> <p> </p> WARNING! <p>Most paint editing software, and also img2img, do not save the metadata. Neither does Discord. When uploading after editing, include the prompt with your image, or it might be gone.</p> <p>Next: 2.2 Add VAE to menu selection</p>"},{"location":"1.%20Installation/2.2%20Add%20VAE%20to%20menu%20selection/","title":"2.2 Add VAE to menu selection","text":"<p>You will end up switching VAEs, whether it's because webui refuses to apply it, or because some models just look better with a different  vae.\u00a0</p> <p>In User Interface, add sd_vae to the Quicksettings list.\u00a0</p> <p></p> <p> This requires a restart to apply. Restarting takes 10 seconds on my laptop. You might want to finish all the changes here.\u00a0 </p> <p>This does that:</p> <p> </p> <p>Click the blue button to refresh models, e.g. if you add in a new model while webui is running, you need to hit refresh to use it in this instance. Alternatively, you can restart everything.</p> <p>Next: 2.3 Set CLIP SKIP to 2</p>"},{"location":"1.%20Installation/2.3%20Set%20CLIP%20SKIP%20to%202/","title":"2.3 Set CLIP SKIP to 2","text":""},{"location":"1.%20Installation/2.3%20Set%20CLIP%20SKIP%20to%202/#anime-models-mostly","title":"(Anime models mostly)","text":"<p>Anime models are typically trained with CLIP SKIP set to 2. It is also doable for non-anime models, but not recommended. </p> <p></p> <p>Next: 2.4 Untick Interrogate Deepbooru sort alphabetically</p>"},{"location":"1.%20Installation/2.4%20Untick%20Interrogate%20Deepbooru%20sort%20alphabetically/","title":"2.4 Untick Interrogate Deepbooru sort alphabetically","text":"<p>If this is standard value, turn it off. It then sorts by weight.</p> <p>Next: 2.5 Tick K-Diffusion Samplers produce same image in a batch</p>"},{"location":"1.%20Installation/2.5%20Tick%20K-Diffusion%20Samplers%20produce%20same%20image%20in%20a%20batch/","title":"2.5 Tick K Diffusion Samplers produce same image in a batch","text":"<p>This is useful for xyz matrix, an extension which I plan on using once I have a better GPU. It allows you to compare similar prompts on the same seed in a table format. </p> <p>Next: Fundamentals of Image Generation Models</p>"},{"location":"2.%20Basic%20Prompting/0.%20Overview/0.%20Table%20of%20Contents/","title":"0. Table of Contents","text":"<p>Prompting Basic Prompting: 1. Fundamentals of Image Generation Models     -   Samplers     -   Tags         - General Tags         - Individual Tags         - Negative Tags     -   Commas     -   Arrangement     -   Brackets and Weights     -   Adjusting CFG     -   Upscalers     -   Target Resolution 1. Improving an existing picture     -   Prompt refining     -   Img2Img     -   Inpainting  1. [[3. Inspiration|Inspiration]]     -   Civitai showcases     -   DeepBooru     -   [[3.3 Danbooru|Danbooru]] Extra     - Textural Embeddings / Textural Inversions     - LoRAs     - Extensions (LoCAN, Couple, More)      - Community</p>"},{"location":"2.%20Basic%20Prompting/0.%20Overview/1.%20Fundamentals%20of%20Image%20Generation%20Models/","title":"1. Fundamentals of Image Generation Models","text":"Notice: <p>If you are just looking to get started and get going, check out Prompting for Dummies. </p> <p>Also, check out Setup. Even people who already got into prompting should make sure that we share similar settings.</p>"},{"location":"2.%20Basic%20Prompting/0.%20Overview/1.%20Fundamentals%20of%20Image%20Generation%20Models/#how-does-a-model-work","title":"How does a model work?","text":"<p>The model has been trained on a Dataset. A model consists of weights that have information about patterns. A Dataset consists of prompts and pictures. </p> <p> When you then query the model with words, it will try to generate the pictures based on the training data. If the prompt contains words that are not in the training data, then it will simply ignore the words, as it doesn't have any pictures or patterns associated with it.  </p> <p> More importantly, the model's view is limited, as its world is limited by the training data. It re-creates patterns that it found in the training data. And much like how humans are quick to assume, the model similarly is often inaccurate, or lacking.</p> <p>Unlike humans, we have immediate ways to combat bias in models. We try to get our desired result anyway. And prompt engineering is a huge part in fighting against bias in a singular model. </p>  For more resources: <p>Layman: CPGrey Math: 3Blue1Brown (check the description for even more links) (has a lot of translations, check the subtitles!) Programming: Codephile</p>"},{"location":"2.%20Basic%20Prompting/0.%20Overview/1.%20Fundamentals%20of%20Image%20Generation%20Models/#what-are-tokens","title":"What are tokens?","text":"<p>A prompt gets broken up into tokens. Tokens are model words, words in the language of the model.</p> <p>As an example, applepie gets broken up into tokens, which could be apple, applepie, and pie, but might also be apple, applepie. </p>"},{"location":"2.%20Basic%20Prompting/0.%20Overview/1.%20Fundamentals%20of%20Image%20Generation%20Models/#what-is-a-prompt","title":"What is a prompt?","text":"<p>A prompt describes all text you prompt the model with. Some people use prompt interchangeably with token, or mean the word(s) inside the commas. In this guide, I will exclusively use it to refer to **the full text input**.  </p>"},{"location":"2.%20Basic%20Prompting/0.%20Overview/1.%20Fundamentals%20of%20Image%20Generation%20Models/#addendum","title":"Addendum","text":"<p>It is important to be aware of the Stable Diffusion base model you are using. Stable Diffusion 2.1 are prompted differently from Stable Diffusion 1.5 models. Most anime models are based on [Stable Diffusion 1.5](&lt;../../Misc/IRL Lore/Stable Diffusion 1.5.md&gt;) and [Novel AI](&lt;../../Misc/IRL Lore/Novel AI or NAI.md&gt;). You should be able to check the version here: </p> <p></p> <p>Next: 1.1 Samplers </p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1%20Samplers/","title":"1.1 Samplers","text":"<p>What a Sampler does doesn't matter for the Basic guide. That is tackled in the Intermediate Guide. No, what matters for us is which Sampler to use. The relevant Samplers for us are: - DPM++ 2M Karras - Euler A - DDIM - UniPC - DPM++ SDE Karras - DPM++ Karras</p> <p>You can set your Sampler and your Sampling Steps here. I will give a range, I myself use the higher end, since regenerating a picture if it's too low steps and doesn't have enough details takes too much time for me.  The way I decide a Sampler is: I check the reviews under the model, and look at what sampler they use. If I use the model more, I will try DPM++ 2M Karras, DDIM and UniPC either way.  </p> <p>Next Sampler: DPM++ 2M Karras Next Chapter:1.2 Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.1%20DPM%2B%2B%202M%20Karras/","title":"1.1.1 DPM++ 2M Karras","text":""},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.1%20DPM%2B%2B%202M%20Karras/#qualitycreativity","title":"(Quality+Creativity):","text":"<p>Note</p> <p> Steps: 28-35. </p> <p>The go-to for creativity and quality. High quality, low performance. Because most current models cannot consistently output high quality, I strongly recommend you to use this sampler to get decent results. Sometimes, other samplers outperform in quality, but in the general case, DPM++ 2M Karras is the go-to. </p> <p>link to pic  It adds details at steps 30-33 that are too small to be visible in this grid.\u00a0</p> <p>Next Sampler: Euler A Next Chapter: 1.2 Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.2%20Euler%20A/","title":"1.1.2 Euler A","text":""},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.2%20Euler%20A/#once-the-best-now-outdated","title":"Once the Best, Now Outdated","text":"<p>Note</p> <p>Steps: 25-40.</p> <p> Useful to check if a model works. Decently fast, lightweight. Good for anime, but every other sampler on this list typically outperform. Some models might be better with Euler A. </p> <p>Older guides might still recommend this. That's because DPM++ 2M Karras didn't exist at that time.</p> <p>Note:</p> <p>Ancestral samplers never settle on a final picture. Don't set your stepcount over 50 unless you like making your GPU burn for no productive reason.</p> <p>link to pic </p> <p>Next Sampler: DDIM Next Chapter: 1.2 Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.3%20DDIM/","title":"1.1.3 DDIM","text":""},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.3%20DDIM/#fast-boring-in-a-good-way","title":"fast, boring, in a good way","text":"<p>Note</p> <p>Steps: 25-33.</p> <p> Fast, sticks to prompt in an ordinary way. In a place where models too creative start drifting away from your prompt or creating unholy terrors, DDIM can offer you consistent quality. It cannot do AND, OR, and similar commands. </p> <p>link to pic </p> <p>Next Sampler: UniPC Next Chapter: 1.2 Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.4%20UniPC/","title":"1.1.4 UniPC","text":""},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.4%20UniPC/#fast-and-quality","title":"(Fast and quality)","text":"<p>Note</p> <p>Steps: 18-25.</p> <p>Really fast while retaining good quality. Has good pictures even at low stepcounts.. I generate pictures in 4 minutes instead of the typical 7 minutes I use for DPM++ 2M Karras.  </p> <p>IMG MISSING</p> <p>Next: DPM++ SDE Karras Next Chapter: 1.2 Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.5%20DPM%2B%2B%20SDE%20Karras/","title":"1.1.5 DPM++ SDE Karras","text":""},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.1%20Samplers/1.1.5%20DPM%2B%2B%20SDE%20Karras/#slow-refined","title":"Slow, refined","text":"<p>Note</p> <p>Steps: 30-37.</p> <p>More elegant/refined feel in the final picture, but takes longer than DPM++ 2M Karras. </p> <p> link to pic </p> <p>Next Sampler: MISSING Next Chapter: 1.2 Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2%20Tags/","title":"1.2 Tags","text":"<p>We established that models are trained using a Dataset. For real life pictures, these often include photos, along with a description of what is happening. For anime pictures, people just use Booru sites, e.g. Danbooru, gelbooru(NSFW warning), or something else. Pictures are already annotated using tags. For the purposes of this guide, a tag describes the words in between commas.  </p> Positive Prompt: <p>1girl, catgirl, meow meow meow meow, cute, </p> <p>\"meow meow meow meow\" would be one of the four tags in this prompt. </p> <p>Similarly, when prompting, you want to write in a tag-style format. That means, instead of prompting \"The moon reflected in a puddle of water their feet, a bystander to the comfort the two friends found themselves in.\", you want to write \"two people hugging, moon reflection on water, warm glow,\", and then pray for the best. </p> I still don't get what \"tag-style\" means. <p>It's a weird kind of caveman speak, where we cut down all words that don't contain information. This makes it appear very brusk, snappy. Example:</p> <p>\"A dog lying on the ground\" -&gt; \"Dog lying on ground\"  -&gt; \"Dog, on ground\"  \"She is wearing a sunflower dress.\" -&gt; \"She wear sunflower dress\" -&gt; \"woman, sunflower dress\"  \"He balls his hands into fists\" -&gt; \"Man, clenched fist\"</p> <p><p>If you want some more concrete tips:</p> <ul><li> avoid \"a, an, the, with,\" and other words that do not contain a lot of information. <li> Keep each tag to a minimum amount of meaningful words. Instead of \"artistic pie with lots of different kinds of fruits\", try \"artistic pie with apple strawberry blueberry\".  <li> Tag about what you want to be present, not what you want not to be present. Instead of \"no hair\", use \"bald\". I know Booru. Can't I just use the tags? <p>If you know what the sites are, you might be tempted to use Booru tags. However, at the beginning, I recommend to only use them sparingly. \"Black hair\" will get understood, even if \"black_hair\" also exists. Just because a tag exists doesn't mean it's good. Furthermore, people can mis-tag in their models.  My main deciding factor is if a particular tag has 800+ images on danbooru. If yes, I will probably use it. If no, I will experiment to see which one works better. A \"black one-piece_swimsuit\" will get understood better than a \"black_one-piece_swimsuit\", because the latter is much rarer in samplesize.</p> <p>Next: 1.2.1 Prompt Structure</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure/","title":"1.2.1 Prompt Structure","text":""},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure/#token-order","title":"Token Order","text":"<p>The model processes [[tokens|tokens]] in batches of 75. Tokens at the front (of the batch) are weighted more. Tokens are processed from left to right.</p> <p>Info</p> <p>Put your complex, picture-defining tags at the start.    </p> <p>Idea</p> <p>Put tokens about the same topic near each other. You can even make paragraphs!</p> <p>Making paragraphs gets ignored in the prompt. </p> <p>This part is so formulaic that I even have a general structure for this:</p> <p>Prompt Structure:</p> <p><ol><li> (Optional:) General description <li> Character(s*)<li> Character pose<li> Background<li> Camera/View<li> Lightning\u00a0<li> Style (can also be first)<li> Boilerplate (e.g. masterpiece, best quality, etc.)<li> Other information (NSFW / SFW) <p>General rule: </p> <p>Complicated tokens to the front &gt; Complicated groups to the front &gt; Structure</p> <ul> <li>For multiple characters, check the Intermediate Guide </li> <li>There are many reasons to lightly deviate from the structure. For example, if the pose is complicated, it might be worth it to push it to the front. However, overall, this structure is recommended.</li> <li>If you want landscape scenes, just skip characters, and add people to the negative prompt.\u00a0</li> </ul>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure/#examples","title":"Examples","text":"<p>It probably is not clear just from the above text what I mean, or maybe the impact is not believable. So, let me go through a few Examples: Grouping is really good: Example 1 - Girl resting in a cave Structure is consistent: Example 2 - City nighttime Headphone Girl Example 3 - Flower shop girl Example 4 - Girl Fights Dragon</p> <p>Prev: 1.2 Tags Next: 1.2.2 Individual Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2%20Individual%20Tags/","title":"1.2.2 Individual Tags","text":"<p>Let us first talk about how to get the most value out of an individual tag. </p> <p>I found the following often true: </p> <p>Info</p> <p>Understandable + Clear prompt =&gt; Consistent quality output  Fewer tokens =&gt; Better token representation</p> <p>However, some models can benefit from more tags; and we also generally want to describe the output well, unless you are doing some experimenting. Generally, we are looking to see 75-150 tokens in the positive prompt. </p> <p>Because of this, we are looking for tags that contain as much information within as few words as possible. The only exception to this is when we have too much information for one batch. In this case, spread the detailed tags among the batches. Refer to the Intermediate Guide for more information. </p> <p>Tips for using individual tags:</p> <ul> <li> Generally, you want to write what you want to see (duh). Don't write what you don't want to see. Especially avoid negatives, e.g. \"No hair\" is worse than \"bald\". The model reads hair, and generates hair. The no is fighting against that generation. This is further covered in [1.3 Token bleed and Token fighting](&lt;./1.3 Token bleed and Token fighting.md&gt;). <li> Use words that you think are represented in the training labeling. As an example, if I want someone mysteriously brooding in front of their computer, I wouldn't use \"programmer\", but \"hacker\". If I want someone chilling in front of a computer, I might use office worker instead. Want someone with a headphone? \"customer service worker\". Recognizing what pictures are associated with each word, and then choosing the right word is a big part of prompting well.  <li> Whatever you don't specify will be left up to the model. Usually, this means they will try to stick to the tag and its associated tags. As an example, if a character typically wears a specific kind of cloth, and you tag that character and don't specify the cloth, the model will typically default to the cloth. It just has the highest probability, because this is represented most in the training data set.  <p>Below, I listed some pretty sensible SFW tags for each section of my prompt structure. They are copied from danbooru. If you are a danbooru user, you will notice that I do not use underscore most of the time.  The reason is that the general tags without underscore were present in the base model, while the underscore tags are only trained on anime pictures. In other words, they are subtly different. The default will give slightly more consistent results (which is what my priority is for this guide) since it went through more training, but sometimes you are actually avoiding the usual tag; in which case the underscore is needed.  This is covered in detail in the Intermediate Guide. For the purpose of the Basic Prompting Guide, we will only sparsingly use actual danbooru tags.</p> <p>If you want to look up booru tags, I recommend this site (Links to NSFW stuff). </p> <p>Legend</p> <p>Just for visual clarity useful to know comment [placeholder]. E.g. [Animal] means that you can insert most common animals in there and it will still understand what you mean. </p> <p>Refer to the whole Tags list folder.  TOC: Overview Start: 1. (Optional) General description</p> <p>Also, here are some really useful adjectives you can use in front of the prompt: E.g. angry eyes, sad smile, etc.  Mood:  warm, cold, comfortable, frightening, horrifying, peaceful, Emotions: happy, cheerful, excited, relieved, affectionate, scared, worried, dreading, angry, hate, annoyed, jealous, disgust, hurt, sad, depressed, disappointed, guilty, surprised, amazed, moved,  Size: tiny, small, medium, large, huge, gigantic, Texture: glossy, oily, wet, metallic, raw, translucent, transparent, see-through, liquid, </p> <p>Next: 1.2.3 Negative Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.3%20Negative%20Tags/","title":"1.2.3 Negative Tags","text":"<p>TL;DR: Use negative tags to fight against positive tags. </p> <p>The best case scenario is when our prompt is not fighting itself. This is not always possible. In these scenarios, you can use other positive tags, or negative tags to tilt the fight into the favor of one side. </p> <p>As an example, you do not want any NSFW in your picture. In this case, adding \"no NSFW\" to the positive prompt is useless, because the model will read and generate \"no\" and then \"NSFW\". The negative prompt works differently from the positive prompt, you can think of it as a prompt subtracting from the positive prompt.</p> <p>It's not nearly as strong with SD 1.5, but it does impact the final picture. </p> <p>Negative prompts are also the place where negative text embeddings should be used. Don't use them in the positive prompt, you have been warned. More on that in the  4.1 Textural Embeddings or Textural Inversions part. </p> <p>This is the negative tags I used to use for SFW pictures. It's old, but it covers most general case scenarios. </p> <p>Negative Prompt:</p> <p>(worst quality, bad quality, normal quality:1.2), nsfw bad anatomy, bad proportions,\u00a0 bad body,\u00a0 bad feet,\u00a0 extra arms,\u00a0 extra legs,\u00a0 bad hands, extra digit, extra fingers, extra limbs, extra digit, fewer digits, fused fingers, gross proportions, malformed limbs, missing arms, missing fingers, missing legs, mutated hands, mutation, polar lowres, poorly drawn face, poorly drawn hands, blurry, cropped, deformed, error, jpeg artifacts, lowres, signature, text, too many fingers, username, watermark,  (bad_prompt_version2:0.4),\u00a0 (easynegative:0.6)</p> <p>Nowadays, I instead start with this prompt: </p> <p> Negative Prompt:</p> <p>(worst quality, bad quality:1.4), </p> <p>And then I add things to it based on the output I get. More on that in the section Improving an existing picture, specifically Prompt refining.</p> <p>The first prompt is perfectly fine though! It is also what I recommend you to use! I use the current one because it is very flexible and I add tags on the go. Negative prompts do not have as much impact on the final picture as the positive prompt.* </p> <p>Next: Token bleed and Token fighting. </p> <p>*If the negative prompt hugely impacts the model, and if you are using the Model Anythingv4 / v4.5, or a model that has it in its mix: Set the target resolution to 512x512, as it sucks at anything else. If it's something else, then head over to 4.1 Textural Embeddings or Textural Inversions, or the Intermediate Guide.  </p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.3%20Token%20bleed%20and%20Token%20fighting/","title":"1.3 Token bleed and Token fighting","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>When playing around with it a little bit, you will find yourself sometimes things sneaking in that you didn't intend to. This usually happens when you combine words for a new meaning, but it registers the individual words itself still. We call this Token Bleed. There are three kinds of Token Bleed:</p> <ol> <li> Cowboy shot generates Cowboys too. =&gt; Multi Word Tags <li> No Muffins generates Muffins too.  =&gt; No Tags <li> Applepies generates Apples too.  =&gt; Multi Syllable Tags <p>I made a checklist for you to go through if you face this issue:</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.3%20Token%20bleed%20and%20Token%20fighting/#case-1-multi-word-tags","title":"Case 1: Multi Word Tags","text":"<p>Step 1: Open danbooru. Step 2: Enter the tag. Step 3 Case A: If there are more than 800 pictures, use the tag. e.g. \"cowboy_shot\" You are now finished.  Step 3 Case B: If there are less than 800 pictures, do not use the tag. Step 4: Look if you can write the prompt differently without spacebar. E.g. Mid shot view Step 5: Put the bleeding word in the negatives. E.g.: Cowboy in negatives  Step 6: Try to see if you can use Weights for the bleeding word, e.g. \"(Cowboy:0.4) Shot\" Use at your own risk, no testing has been done on my part. I will test once I get new GPU Step 7: Write the words together without leaving a spacebar. E.g.: \"cowboyshot\"  Step 8: Pray.</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.3%20Token%20bleed%20and%20Token%20fighting/#case-2-no-tags","title":"Case 2: No tags","text":"<p>Step 1: Describe what you want without using any negation, e.g. \"no hair\" -&gt; \"bald\". If this works, you are finished. Step 2: Remove it from the positive prompt and add it without the \"no\" to the negative prompt. Step 3: Pray.</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.3%20Token%20bleed%20and%20Token%20fighting/#case-3-multi-syllable-tags","title":"Case 3: Multi Syllable Tags","text":"<p>Step 1: Step 1: Open danbooru. Step 2: Enter the tag. Step 3 Case A: If there are more than 800 pictures, use the tag. e.g. \"apple_pie\" You are now finished. Mind you, bleed can still exist, it's just a little bit less. Step 3 Case B: If there are less than 800 pictures, do not use the tag. Step 4: Look if you can write the prompt differently without multiple joined words, e.g. \"pie\" instead of \"applepie\".  Step 5: Put the offendingword in the negatives. Step 6: Pray.</p> <p>Alternatively, if the above doesn't work, check out some 4.3 Extensions.</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.3%20Token%20bleed%20and%20Token%20fighting/#token-fighting","title":"Token Fighting","text":"<p>In the previous section, we put the word in the negatives if we did not want it to be in the output. In these cases, the word in the negatives is fighting against the positive prompt and the bias of the model. </p> <p>The tags in the positive prompt can also fight among each other. If you get inconsistent results, watch out for tokens that can fight each other. If you instead want to combine the tokens, check the Intermediate Guide!</p> <p>Next: 1.4 Weights</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure%20Examples/Example%201%20-%20Girl%20resting%20in%20a%20cave/","title":"Example 1   Girl resting in a cave","text":"<p>As an example, instead of:\u00a0</p> <p>Positive Prompt</p> <p>1girl, blue cave, dim light, comfortable, lying down, smile, crystal, deep eyes,\u00a0</p> <p>Do:</p> <p>Positive Prompt</p> <p>1girl, lying down, smile, deep eyes,\u00a0 blue cave, crystal, dim light, comfortable,\u00a0</p> <p>The model might still execute badly. As an example, the blue from the blue cave can bleed into the girl's dress or something like that. Preventing bleed is more detailed in the  Intermediate Guide. However, the good prompt is still a lot less confusing to the model. </p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure%20Examples/Example%201%20-%20Girl%20resting%20in%20a%20cave/#heres-how-the-model-processes-the-prompt-in-the-bad-prompt-the-model-goes-through-the-prompt-and-on-the-8th-token-knows-the-overall-composition-by-that-time-the-structures-for-1girl-blue-cave-dim-light-and-comfortable-were-already-processed-it-then-is-told-that-there-is-a-crystal-and-that-the-girl-is-smiling-almost-every-token-affects-a-different-area-in-the-good-prompt-the-model-goes-through-the-prompt-and-on-the-third-token-prepares-its-structures-for-lying-down-that-is-the-composition-roughly-that-it-will-go-for-it-then-adds-details-to-the-girl-first-then-to-the-background-the-tags-affecting-the-same-area-apply-at-roughly-the-same-time","title":"Here's how the model processes the prompt:   <p>In the bad prompt, the model goes through the prompt, and on the 8th token knows the overall composition. By that time, the structures for 1girl, blue cave, dim light, and comfortable were already processed. It then is told that there is a crystal, and that the girl is smiling. Almost every token affects a different area. </p><p>In the good prompt, the model goes through the prompt, and on the third token prepares its structures for lying down. That is the composition roughly that it will go for. It then adds details to the girl first, then to the background.\u00a0The tags affecting the same area apply at roughly the same time. </p>","text":"<p>Next: Example 2 - City nighttime Headphone Girl</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure%20Examples/Example%202%20-%20City%20nighttime%20Headphone%20Girl/","title":"Example 2   City nighttime Headphone Girl","text":"<p>We have this prompt from a picture on the MeinaMix gallery:</p> <p>Positive Prompt:</p> <p>(extremely detailed CG unity 8k wallpaper, masterpiece, best quality, ultra-detailed, best shadow), beautiful detailed glow, cowboy shot, cinematic lighting, 1girl, (beautiful detailed face, beautiful detailed eyes), Mischievous eyes, blue eyes, happy expression, full lips, black pink jacket, hood jacket, (white shirt),\u00a0 shorts, legs, (wear headphone:1.4), knee-high socks, normal breast, pink hair, shoulder-length hair, hair cover ears, short hair, dynamic pose, lithe form, five fingers, cinematic, proper fingers, city background</p> <p>This is hard to read. First, we are going to add paragraphs and group it.\u00a0</p> <p>Positive Prompt:</p> <p> Boilerplate (extremely detailed CG unity 8k wallpaper, masterpiece, best quality,\u00a0ultra-detailed, best shadow), Lightning beautiful detailed glow, Camera cowboy shot,  Lightning cinematic lighting,  Character\u00a0 1girl, (beautiful detailed face, beautiful detailed eyes), Mischievous eyes, blue eyes, happy expression, full lips, black pink jacket, hood jacket, (white shirt),\u00a0 shorts, legs, (wear headphone:1.4), knee-high socks, normal breast, pink hair, shoulder-length hair, hair cover ears, short hair,  Pose dynamic pose, Character lithe form, five fingers, Lightning cinematic,  Character proper fingers,  Background city background </p> <p>Notice how the layers are all over the place?\u00a0</p> <p>Let's try grouping it using my format above.\u00a0</p> <p>Positive Prompt:</p> <p>Character 1girl, (beautiful detailed face, beautiful detailed eyes), Mischievous eyes, blue eyes, happy expression, full lips, black pink jacket, hood jacket, (white shirt),\u00a0 shorts, legs, (wear headphone:1.4), knee-high socks, normal breast, pink hair, shoulder-length hair, hair cover ears, short hair, lithe form, five fingers, proper fingers,  Pose dynamic pose,  Background city background, Camera cowboy shot,  Lightning beautiful detailed glow, cinematic lighting, cinematic,&gt; Boilerplate (extremely detailed CG unity 8k wallpaper, masterpiece, best quality,\u00a0ultra-detailed, best shadow) </p> <p>This is much more understandable! The model can understand this better, and so do we! We didn't even fix the tags yet! We'll tackle that in the tags section.\u00a0</p> <p>Let us actually analyze what the changes did, using the same seed:</p> <p>Prompt from user of the site:</p> <p>link to pic </p> <p>Adjusted prompt order:</p> <p>link to pic </p> <p>Note:</p> <p>If you have troubles comparing these, open the links, and swap between the tabs (Firefox: Ctrl + Up / Down arrow). Concentrate on one picture at a time.</p> <p>Comparison:</p> <ul> <li> Some values, like the white shirt, are more represented in our generations compared to theirs.  <li> The expression itself looks a bit more doll-like (which can be because of the CG token), which we didn't really specify in both prompts.  <li> The \"hair covers ears\" prompt was not respected by any generation.  <li> I'd say that our generations look a bit more cinematic, but honestly there is only a minor improvement; but if you create a batch of a hundred pictures, our prompt will make a number of them closer to the prompt.\u00a0  <p>Next: Example 3 - Flower shop girl</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure%20Examples/Example%203%20-%20Flower%20shop%20girl/","title":"Example 3   Flower shop girl","text":"<p>We have another prompt from the MeinaMix gallery:</p> <p>Positive Prompt:</p> <p>(best quality), ((masterpiece)), (highres), extremely detailed girl, solo, sharp focus, ((((((flower shop)))))), ((cinematic lighting)), (((character(((1 girl)))))), solo, (((beautiful plain pink t shirt))), pants, standing, light smile, closed mouth, (((((sharp focus))))), (((((masterpiece illustration))))), ((((medium shot)))), shiny hair, short hair, blonde hair, curly hair, hair between eyes, beautiful detailed eyes, blue eyes, jewel-like eyes, diamond-shaped pupils, large breasts, animal ears, beautiful detailed jewel leather collar, backpack, black legwear,</p> <p>Broken up:</p> <p>Positive Prompt:</p> <p>Boilerplate (best quality), ((masterpiece)), (highres), extremely detailed girl, solo,  Camera sharp focus, Background ((((((flower shop)))))),  Lightning ((cinematic lighting)),  Character (((character(((1 girl)))))), solo, (((beautiful plain pink t shirt))), pants, standing, light smile, closed mouth, Camera (((((sharp focus))))),  Boilerplate (((((masterpiece illustration))))),  Camera ((((medium shot)))),  Character shiny hair, short hair, blonde hair, curly hair, hair between eyes, beautiful detailed eyes, blue eyes, jewel-like eyes, diamond-shaped pupils, large breasts, animal ears, beautiful detailed jewel leather collar, backpack, black legwear,</p> <p>Oof that's kinda all over the place. The person still got a good picture, but I do wonder how many bad pictures it also generated. Especially with this many colour prompts, which have a tendency of bleeding...\u00a0</p> <p>Let's clean it up a little.\u00a0</p> <p>Positive Prompt:</p> <p>Character solo, (((character(((1 girl)))))), solo, (((beautiful plain pink t shirt))), pants, standing, light smile, closed mouth, shiny hair, short hair, blonde hair, curly hair, hair between eyes, beautiful detailed eyes, blue eyes, jewel-like eyes, diamond-shaped pupils, large breasts, animal ears, beautiful detailed jewel leather collar, backpack, black legwear, Background ((((((flower shop)))))),  Camera sharp focus, (((((sharp focus))))), ((((medium shot)))),  Lightning ((cinematic lighting)), Boilerplate (((((masterpiece illustration))))), (best quality), ((masterpiece)), (highres), extremely detailed girl,</p> <p>We can already see a lot of repetitive tags again. Also, the bracket usage is quite hard to adjust. We will detail why and what to do instead in the bracket section. For now, let's just keep it as is. </p> <p>Let's look at the generations again :3</p> <p>First, the old prompt:</p> <p>link to pic </p> <p>Now, the adjusted prompt: </p> <p>link to pic  </p> <p> Overall, our adjusted prompt respects cinematic lightning a lot more. The mouth is also consistently closed. The hair style is relatively consistent, but for some reason ours is completely missing the \"beautiful detailed jeweled collar\". I'm not sure why, but it might be too far to the back for it to properly add. The other prompts might also steal too much attention from this token. To fix this, I would move both the backpack and the jewelry accessory to the front, and I would adjust the weights appropiately. </p> <p>Next: Example 4 - Girl Fights Dragon</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.1%20Prompt%20Structure%20Examples/Example%204%20-%20Girl%20Fights%20Dragon/","title":"Example 4   Girl Fights Dragon","text":"<p>Lastly, from the MeinaMix gallery: A girl fights a dragon.</p> <p>Positive Prompt</p> <p>(best quality) (masterpiece), 1girl, solo, epic battle, fire magic, dragon, smoke, flames, sparks, flying debris, intense action, dramatic lighting, dark sky, determination, concentration, wizard hat, robe, staff, fierce expression, powerful stance, adrenaline, magic aura, claws, scales, fire breath, danger, fear, bravery, victory, five fingers, silver hair</p> <p> First step: Break it up</p> <p>Positive Prompt</p> <p>Boilerplate (best quality) (masterpiece), character1/boilerplate 1girl, solo,  General Description epic battle, Character1? Forget about the model, even I am confused who this should apply to. fire magic,  Character 2 dragon Background smoke, flames, sparks, flying debris, Mood/Style intense action,  Lightning dramatic lighting, dark sky, Character1??? determination, concentration, Character1 wizard hat, robe, staff, fierce expression, powerful stance, adrenaline, magic aura,  Character2 claws, scales, fire breath,  Character1 danger, fear, bravery, victory, five fingers, silver hair</p> <p>Now we rearrange it:</p> <p>Positive Prompt</p> <p>General Description* epic battle, 1girl fights dragon, fire magic,  Character1 1girl, solo, wizard hat, robe, staff, five fingers, silver hair, fierce expression, powerful stance, adrenaline, magic aura, determination, concentration, victory, danger, fear, bravery,  Character2 dragon, claws, scales, fire breath, Style/Mood intense action,  Background smoke, flames, sparks, flying debris, Lightning dramatic lighting, dark sky,  Metadata (best quality) (masterpiece), </p> <p>Multiple characters are hard.</p> <p><p>They are an entire section in the Intermediate Guide. I added one prompt, \"1girl fights dragon\", to generally describe the scene. Otherwise, it would say \"1girl, dragon, epic battle\" and I think that's too confusing for the machine. \u00a0<p></p> <p>Mood and emotion certainly can belong to style. More details when we go down to individual tagging. Here, I already anticipated it to be harder to execute, so I pushed it more to the front.\u00a0</p> <p>I also did some light intermediate prompting for two characters, resulting in this:</p> <p>Positive Prompt</p> <p>General Description* epic battle, 1girl fights dragon, fire magic, BREAK, Character1 1girl, solo, wizard hat, robe, staff, five fingers, silver hair, fierce expression, powerful stance, adrenaline, magic aura, determination, concentration, victory, danger, fear, bravery, BREAK,  Character2 dragon, claws, scales, fire breath, BREAK, Style/Mood intense action,  Background smoke, flames, sparks, flying debris, Lightning dramatic lighting, dark sky,  Metadata (best quality) (masterpiece), </p> <p>Essentially, I added a few BREAK tags to make sure that Character 1 and Character 2 are processed at different times by the model. I will go more in depth on the BREAK command in the Intermediate Guide. For now, just know that it makes character 2 more consistent.\u00a0</p> <p>Let's look at some pictures :D</p> <p>Original:</p> <p>link to pic </p> <p>Adjusted:</p> <p> link to pic </p> <p>Adjusted with BREAK: </p> <p>  link to pic  </p> <p>Our adjustments made the \"wizard hat\" consistently appear. The staff does look better, but that might be more chance than good prompting (weapons, on a non-specialized model or LoRA, are typically hard.) There are more flames, and they look a lot more blazing. Because of the staff, using fire magic with hands is a lot less prevelant. Instead, we see random, staff-like streaks of flames cross the picture. In none of the pictures did the dragon breathe fire. (Might require better tagging / prompting or more images).\u00a0</p> <p>The pictures that we made using BREAK have distinctly more dramatic lightning, and a higher chance of there being one dragon. The dragon and the wizard are not necessarily fighting each other (likely caused by inprecise general prompt) but at least the dragon appears more consistently. In none of them does the character look like they won the fight. There also is no fire aura. \"victory\" as a prompt might just be too ambiguous. </p> <p>Overall we noticed these changes:</p> <ul> <li> A higher chance that each tag in the prompt is represented <li> A higher impact of tags than before  And, most importantly: <li> Much easier to read for humans.  <p>If you are trying to edit or improve a prompt, it helps if you only need to pay to one line instead of trying to look everywhere what you already have added and what is still missing. </p> <p>In this section, we dealt with how to structure your prompt. In the next section, we will talk about which tags to use and which ones not. </p> <p>Next: 1.2.2 Individual Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/1.%20%28Optional%29%20General%20description/","title":"1. (Optional) General description","text":"<p>No tags available for this section. This is the part that most resembles standard prompting. You write a short descriptive phrase to give an overview to the model.  Write something like \"1girl fights against dragon\", \"2girls holding hands in moonlight\", \"three girls sitting in cafe\". You still want to minimize tokens, so cut out the \"a\", \"the\", and only add prepositions you know you need.  E.g.: An earlier prompt in this doc which we improved on \"1girl fights dragon\", but it often ends up with the girl fighting WITH the dragon, so the preposition AGAINST is needed.</p> <p>Next: [[0. Overview + 7z file|2. Character Overview]]</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.5%20Pose/","title":"2.5 Pose","text":"<p>2.5 Character Pose whole body: kneeling, lying, (+ crossed legs, back, side, stomach), sitting, (+ crossed legs, hugging own legs, lotus position, sitting on lap), standing, (+ balancing, crossed legs, legs apart),\u00a0 walking, running, crawling, balancing, wallrunning (tbh a bit iffy), jumping,\u00a0 cowering, all fours, crucifixion, faceplant, fighting stance, squatting, yoga + ballet poses can help but they are more useful for realistic models, also use ControlNet, covered in Extensions and the Intermediate Guide. arm support, head rest,\u00a0 aiming, [object] hug, hugging [object], hugging own legs, arm hug, hug from behind, carrying (over shoulder / under arm), princess carry, shoulder carry,\u00a0 archer pose, victory pose, villain pose, zombie pose,\u00a0</p> <p>head: head down, head tilt</p> <p>torso: arched back, bent over, leaning forward, leaning back,\u00a0</p> <p>arms: arms behind back, arm up, arm behind head, victory pose, arms up, \\o/, outstretched arm, arm at side crossed arms, flexing, reaching, thinking pose</p> <p>legs + lower: crossed ankles, folded legs, leg up, legs up, knees to chest, leg lift, standing split,\u00a0</p> <p>two+ characters: back-to-back, cheek-to-cheek, eye contact, forehead-to-forehead, heads together, holding hands, group hug, circle formation,</p> <p>gestures: air quotes, finger counting, own hands clasped, high five, extending, pointing (at [object]), shushing, thumbs up, thumbs down, ok sign, money gesture, claw pose, open hand, facepalm, handshake, waving, beckoning,\u00a0</p> <p>At a certain point I recommend using ControlNet, covered in Extensions and Intermediate Guide. You can also use LoRAs.</p> <p></p> <p>Next: 3. Background</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/3.%20Background/","title":"3. Background","text":"<p>Here, dumping details is actually pretty ok to make a picture appear more natural (as long as not every detail is important). For example, most bedrooms have a form of light in there, but anime generated pictures do not necessarily have them. Adding \"lamp, lights, lights on, windows,\" relatively late into the prompt can help the model add details to make it seem realistic. If you are stumped for details, you can ask ChatGPT, or look up pictures on the internet, then name what you see then throw it into CLIP or DeepBooru in the img2img tab. Alternatively, you can just leave it up to the model, but it's kinda random what you get. I actually know a lot less about this; if someone has extensive experience with background / landscape prompting, I'm very interested to hear your thoughts. </p> <p>Here's a list of basic backgrounds and some objects they might have (do start with e.g. background, forest):</p> <p>[colour] background, simple background,\u00a0</p> <p>City street / city background: [insert name of city], crowd, [adjective] streets, traffic lights, pylon, skyscraper, trains, cars, trucks, bridge, trees,\u00a0</p> <p>Living room: fireplace, couch, lamp, wood, chair, table, book shelves, rug, windows, door window, potted plants, ornament shelves, plate shelves, XX era,</p> <p>Forest: trees, bamboo, grass, hills, valley, spring, summer, autumn, winter, snow, ice, rain, dew, river, stream, lake, pond, wildlife, [animal],\u00a0</p> <p>Some more sceneries: cave, cyberpunk, machinery, abstract, bedroom, office, fantasy,</p> <p>There's still a section on this in the Intermediate Guide, part of the \"fighting against bias\" section.\u00a0</p> <p></p> <p>Next: 4. Camera + View</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/4.%20Camera%20%2B%20View/","title":"4. Camera + View","text":"<p>Camera/View In order of increasing view:  face (Face mostly, can be upper shoulder) portrait (Face through shoulders) (avoid if looking for anything else than a game portrait, instead use close-up face) upper body (Face through torso) lower body (From torso down) cowboy_shot (Face through thighs) (use midshot, or copy it with the underscore. This one is currently the only exception. Full explanation in  Token Bleed .) mid shot (Face to below knee)\u00a0(ppl mistagged, so it can also generate similar to cowboy_shot) full body (Whole body) wide shot (Whole body from far away) very wide shot (Whole body from very far away) landscape (needed for landscape or ppl will appear)</p> <p>Also: close-up (can also be used on face for what portrait should do) cut-in top down view / from above, from below, back view, side view, dutch angle shot, fish eye shot,\u00a0 dutch_angle (tilted picture for dramatic effect, copy it with the underscore.) profile (side profile)</p> <p></p> <p>Next: 5. Lightning + Focus</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/5.%20Lightning%20%2B%20Focus/","title":"5. Lightning + Focus","text":"<p>Light, glow: backlighting (light from behind character, e.g. blinds, looks really good for dramatic scenes), bloom (makes glow glow more),  diffraction spikes (single point light spike, e.g. twinkling star that shines brighter), lens flare, tyndall effect (light scatters around),\u00a0 light rays, ray tracing (makes rays more prominent, and does some other stuff that resembles the ray tracing effect),\u00a0 god rays (the light that appears when something divine happens), dappled light (round lights that peek through e.g. foliage of the forest),\u00a0 light glare,\u00a0\u00a0 overexposure (brighter image),\u00a0 reflection,\u00a0 caustics (water light reflection), refraction (think of a prism breaking light),\u00a0 hard light (light stands out from shadow harder), warm glow,\u00a0 glowing white particles,\u00a0</p> <p>shadow:  hdr (make dark things darker, bright things brighter, contrasting light and shadow, typically creates light in middle darkness outside), shadow,\u00a0 dropshadow (character shadow is thrown hard onto wall), vignette (outside dark inside bright),</p> <p>focus: bokeh (things out of focus e.g. background / edge of image get more blurry), depth of field (certain layers become blurry), blurry foreground, blurry,\u00a0 blending (obj blends into BG, bleeding seems to get closer and blending doesn't seem to work at all, likely because of Token bleed),</p> <p>Misc: chiaroscuro (red-blue colour contrast for 3D volume), chromatic aberration (motion somewhat, also lens, a bit more \"real\"), (I need to check if I maybe swapped out the two I don't have internet rn) emphasis lines (anime emphasis) / motion lines, double exposure (layer one image inside the other image, MeinaMix CANNOT do this, better to get the LoCan on Civit / the LoRA if such a LoRA is out), For even darker shadows, see the Intermediate Guide</p> <p></p> <p>Next: 6. Style</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/6.%20Style/","title":"6. Style","text":"<p>If you know an anime show that you like, or artists that you like, or if it's even a decade of Anime done in that style, I recommend that you try to type that in, or look around if there is a LoRA or even a model done for that style. More details in the LoRA section, basically, if I personally want a style, I'm looking for the right model and LoRA combination to get it generally, instead of typing in shows the model might not even be trained on.</p> <p>Colors: Keep US spelling impasto (very colourful), vivid colors, limited palette, pastel colors, monochrome, duochrome,  RAW (doesn't work that well, instead use the following two:), brutal, grotesque\u00a0(Both prompts need an actual prompt for context or it suffers from token bleed)</p> <p>Lines: lineart, sketch, rough sketch, outline, pixel art, greyscale, 3D render, octane render, CG,\u00a0Unreal Engine, CGStation, (for these, get a LoRA:) thick outline, white outline, calligraphy_brush, color trace, </p> <p>Format: illustration, portrait, cover, wallpaper,  Realism: DSLR photo, the lens of the camera (e.g. 200mm, 21mm, works less well on anime models but can still work)</p> <p>Anime vs Real Style:\u00a0 realism, painterly, pastel, anime, </p> <p>Other: [Setting], [Country], [epoch], traditional/modern, [image style], [artist], [show], [Setting] might even be worth pushing to the front. Meina is not super flexible with painting styles, because it's mostly trained on anime images. You can grab different models for different styles, as well as LoRAs.</p> <p></p> <p>Next: 7. Boilerplate + Misc</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/7.%20Boilerplate%20%2B%20Misc/","title":"7. Boilerplate + Misc","text":"<p>General: intrinsically detailed (makes more details / clutter appear),   extremely detailed,  same as (intrinsically detailed:1.5) masterpiece, best quality (does little, but is not intrusive and generally makes it sliiiightly better). The closer to older models you are, the more likely you should put this at the end of your prompt. 4k (realism, can help structure it in a sharper / more powerful way),\u00a0</p> <p>Hair, Face, chest, belly, thigh, legs feet,  (add at end of character to control what the picture views) There is no boilerplate that helps with hands. As far as I know. Deep eyes, (makes it so eyes get slightly more focus)</p> <p>Misc:  NSFW / SFW</p> <p></p> <p>Next: 1.2.3 Negative Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/0.%20Overview%20%2B%207z%20file/","title":"0. Overview + 7z file","text":"<p>1. Amount + Gender 2. Hair 3. Face 4. Eyes 5. Ears 6. Body Figure 7. Skin Color Attire 8.1 Head 8.2 Top 8.3 Bottom 8.4 Legs, Feet, Shoes 8.5 Misc, onepiece, costume 8.6 Traditional Clothes [[8.7 Misc + onepiece]] 8.7 Accessories</p> <p>.7z file of all the pics. 7z is a different kind of zip. The advantage is that it cannot be resolved as a link as of 02.06.2023 and hopefully Google doesn't ever do this again. </p> <p>Next: 1. Amount + Gender</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/1.%20Amount%20%2B%20Gender/","title":"1. Amount + Gender","text":"<p>aAmount + Gender:  1girl, 1boy, 1other (e.g. monsters), 2girls*, solo \u00a0</p> <p>Note:</p> <p>Multiple people is hard, there is an entire section dedicated to that in the Intermediate Guide</p> <p>Also, 1other just makes it easier to prompt non-humans, it unfortunately doesn't exclude humans.</p> <p></p> <p>Next: 2. Hair</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/2.%20Hair/","title":"2. Hair","text":"<p>length:  bald hair, short hair, medium hair, long hair</p> <p>style:  braids, bun, ponytail, twintails, afro</p> <p>texture:  curly hair, drill hair, messy hair, straight hair, wavelets, </p> <p>sticking out:  bangs, ahoge, combed, hair pulled back,\u00a0</p> <p>hair up, hair down</p> <p></p> <p>Next: 3. Face</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/3.%20Face/","title":"3. Face","text":"<p>emotional faces:  angry, annoyed, blush, bored, closed eyes, crazy, disappointed, disgust, despair, expressionless, flustered, grimace, winking (unreliable), pout &lt;(Don't use pout, use :&lt; instead. It just looks wrong idk what's going on here), sad, tears, scared, sigh, surprised, upset, smile, grin, smug,\u00a0</p> <p>emotes:  :o, :3, uwu, owo, o_o, :&lt; and some other emotes work. They need to be recognized text emotes. Yes, someone looked at a picture and annotated it with uwu.\u00a0Other emotes don't work. Just experiment with it.</p> <p>Also: tongue, tongue out,\u00a0</p> <p>If you are looking to get a different face from the usual faces, I recommend using names and random characters for more variation. This is further covered in the Intermediate Guide.</p> <p></p> <p>Next: 4. Eyes</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/4.%20Eyes/","title":"4. Eyes","text":"<p>[!info] Hexcodes do not work. More in the colour section, in the Intermediate Guide.\u00a0</p> <p>[color] eye:  (blue, aqua, black, etc.  Even just googling for what the colour is called helps. I've seen someone use \"baby pink\", then change it to \"pale pink\" to prevent bleed from \"baby\".  Colour is a strong prompt. I recommend keeping eye weight at 0.4 to prevent leakage. A LOT of colours work here. </p> <p>Misc: \"multicolored eyes\", heterochromia\u00a0 colour + pupil colour + sclera bags under eyes, glowing eyes, crazy eyes, text emotes also work for eyes like \"&gt;&lt;\" (you might need a specific LoRA for that). hair over both eyes, hair over one eye, blindfold, eyepatch, glasses, </p> <p></p> <p>Next: 5. Ears</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/5.%20Ears/","title":"5. Ears","text":"<p>Ears: Animal Ears:  cat ears, dog ears, fox ears, wolf ears, fake animal ears, electric rat ears (use it uncensored, I just don't want Nintendo to find this doc),\u00a0</p> <p>Other:  pointy ears,\u00a0</p> <p></p> <p>Next: 6. Body Figure</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/6.%20Body%20Figure/","title":"6. Body Figure","text":"<p>upper torso: neck, long neck (good negative), back, collarbone, (small/medium/large) breasts, pectorals, ribs (good negative), broad shoulders, nape, shoulder blades, bare shoulders, off shoulder,</p> <p>lower torso:  (exposed) stomach, abs, belly, navel, (flat / huge) ass, thighs, hips, waist, </p> <p>appendages:  arms, feet, hands, knees, legs, tail, tentacles, toes, wings,</p> <p></p> <p>Next: 7. Skin Color</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/7.%20Skin%20Color/","title":"7. Skin Color","text":"<p>Skin Color: dark skin, pale skin, tan, [color] skin also works. </p> <p>(If you are trying to generate people of colour, there's a separate guide below in the Intermediate Guide. Anime very obviously just doesn't have a lot of people of colour images. It's still possible though, and much easier if you use a more diverse model like MothMix or a LoRA instead.)</p> <p></p> <p>Next: 8.1 Head</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.1%20Head/","title":"8.1 Head","text":"<p>Hats on head Casual hat, helmet, cap, cowboy hat, straw hat, fedora, bicycle helmet, motorcycle helmet,[animal] hat,  Role-hat witch hat, wizard hat, nurse cap, santa hat, Crown, circlet, diadem, tiara,</p> <p>Wearable object on head [object] on head,\u00a0 camouflage helmet, military hat / helmet, police hat,</p> <p>Misc:  hair bow, hair ribbon, hairband, headband, veil, [object] in hair, [object] hair/hat ornament,</p> <p>For Asian hats, check out danbooru. </p> <p></p> <p>Next: 8.2 Top</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.2%20Top/","title":"8.2 Top","text":"<p>neutral:  coat, overcoat, raincoat, winter coat,  hoodie, jacket, suit jacket,leather jacket, sash, shirt, collars, sleeveless, t-shirt, sweater, pullover, tank top, vest,</p> <p>trad female:  blouse, crop top, cardigan, compression shirt, corset, dress, frills, off-shoulder shirt, bra,</p> <p></p> <p>Next: 8.3 Bottom</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.3%20Bottom/","title":"8.3 Bottom","text":"<p>Bottom bloomers, buruma, pants, jeans, shorts, skirt,</p> <p></p> <p>Next: 8.4 Legs, Feet, Shoes</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.4%20Legs%2C%20Feet%2C%20Shoes/","title":"8.4 Legs, Feet, Shoes","text":"<p>Legs + Feet: leggings, stockings, pantyhose, socks,\u00a0</p> <p>Shoes: Boots, sandals, slippers, sneakers, high-heels</p> <p></p> <p>Next: 8.5 Misc, onepiece, costume</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.5%20Misc%2C%20onepiece%2C%20costume/","title":"8.5 Misc, onepiece, costume","text":"<p>Misc / one-piece: apron, armor, cape, pajamas, overalls, swimsuit, bodysuit, jumpsuit, leotard, robe,</p> <p>Costume: cheerleader, ghost costume, gym uniform, [animal] costume, maid, military uniform, pilot suit, school uniform (dw not Chinese ones, unless you want to), sailor dress, suit, business suit, waitress, cowboy,</p> <p></p> <p>Next: 8.6 Traditional Clothes</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.6%20Traditional%20Clothes/","title":"8.6 Traditional Clothes","text":"<p>chinese:  changpao, china dress, fengguan, hanfu, longpao, tangzhaung</p> <p>japanese:  geta, hakama, kimono, haori, miko, sarashi, </p> <p>korean:  hanbok</p> <p>If you have any suggeston on western traditional clothes I'm open to add them. MeinaMix needs to recognize them though. If you have other traditional clothes you want to instead copy, I recommend using a different model that's trained on this. </p> <p></p> <p>Next: 8.7 Accessories</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.2-1.3%20Tags/1.2.2.1%20Tags%20list/2.%20Character/8.%20Attire/8.7%20Accessories/","title":"8.7 Accessories","text":"<p>Accessories typically don't count towards the clothes or naked prompt</p> <p>Head + Face: Earrings, glasses, hair beads, hairclip, hairpin, mask,</p> <p>Neck + Shoulder: bowtie, choker, collar, necklace, necktie, scarf,\u00a0</p> <p>Limbs: armband, armlet, bracelet, detached sleeves, fingernails, gloves, ring, wristband, wrist cuffs,\u00a0</p> <p>Torso: Belt,\u00a0</p> <p>Misc: Badge, buttons, piercing, tattoo, watch,\u00a0</p> <p>Patterns: Camouflage, checkered, plaid, polka dot, striped, [object] print,\u00a0</p> <p>Misc: Frills, trim, see-through [cloth], torn [cloth]</p> <p></p> <p>Next: 2.5 Pose</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.4-1.9%20Everything%20surrounding%20words/1.4%20Weights/","title":"1.4 Weights","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>Weights are used to make the model pay more attention to the relevant word. As an example: (Cat:1.5). The numbers are a multiplier. Anything between 0 and 2 is recommended, a weight beyond 2 is NOT recommended. </p> <p>It has this structure, and this structure ONLY:</p> <p>(Your Tag:Weight) Please note how the weight is INSIDE the bracket, and AFTER the colon.</p> <p>If you have a bracket for multiple words, e.g. (Long hair, drill hair:1.2), the weight is applied to both tags. </p> <p>If you do not use any brackets, then the weight is 1. If you use () round brackets, the weight inside the bracket is multiplied by 1.1. ((This)) is 1x1.1x1.1 = 1.21 weight. </p> <p>I personally use 1.2 / 1.3 at the start if I think it's underrepresented, and 0.8 if it's overrepresented, and then I finetune it. The typical maximum I go for is 1.5, and the typical minimum 0.4. If I go over / beneath that, that already feels uncomfortable to me, but it can be necessary. Do not go over a weight of 2. You will start seeing only patterns.</p> <p>These weight guidelines are for standard tags only. </p> <p>For LoRAs I recommend a standard value of 0.7-0.8, and then go down if needed. More often than not, I will go down to 0.6, 0.4, or even 0.2. The problem with LoRAs at high weights are blurry faces, bad hands, and art style changes. You still want to take LoRAs because of their more specialized benefits, and they will still apply at lower weights.  Check the LoRA page to see what the uploader recommends!</p> <p>For |Textural Embeddings, I recommend a standard value of 0.6, or 0.4 for stronger TIs. Most people have settled on 0.8 and 0.6, but that actually still is too strong. Some people think that Textural Embeddings cannot harm the prompt unlike LoRAs \u2014 and these people are wrong. A Textural Embedding that is too strong can and will harm your output. If you see weird glitches or artifacts, that might be caused by a TI that is too strong. </p> <p>For Hypernets, I have no clue, because I haven't ran Hypernets before. I will do so once I get a better GPU. </p> <p>Next: 1.5 Cohesive Tags</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.4-1.9%20Everything%20surrounding%20words/1.5%20Cohesive%20Tags/","title":"1.5 Cohesive Tags","text":"<p>So far, we have covered individual tags alone. However, tags function differently when working together. Some tags will give context for other tags and make an outcome much more likely. For example, the lying locust pose will generate a decent amount of locust, unless you add the context that it is training in a different tag. </p> <p>A common way to do it is to find a tag that combines both the thing you want and the context. For example, cowboy_shot is a tag that is mostly used to denote the camera view associated with it on Danbooru, different from cowboy shot, which can also be a shot from or of a cowboy.</p> <p>You can use these danbooru tags if you think they are represented well in the training data; this typically means at least 800 pictures, but can also work with less. The best way to tell is to just try to generate the tag and observe the influence of each tag. </p> <p>Alternatively, you can use more text to tie tags more cohesively together. Here is a few forms of that: (pillow girl), pillow+girl, pillow_girl, pillowgirl (just writing it together grammar be darned). I recommend not combining more than 5 words this way; CLIP might otherwise just ignore it. A differentiation between each of these will be eventually done in Commas, as part of the Intermediate Guide. </p> <p>There is a much more general differentiation you can use now however:</p> <p>Info</p> <p>The weakest on the left.  \"random, things\", random things &lt; (random things), random+things, random_things, randomthings &lt; (notrandomthing), notrandomthing, not_random_thing,  In other words, it's worth trying to look for a word combination that work via the words themselves, over mashing random words together.</p> <p>As an example, \"red eyes\" is a common combination, so switching to red+eyes / redeyes would be worse. However, \"lizard keyboard\" is more uncommon, and might get processed separately instead, so switching to \"lizardkeyboard\"/\"lizard+keyboard\" is better.</p> <p>Next: 1.6 Adjusting CFG</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.4-1.9%20Everything%20surrounding%20words/1.6%20Adjusting%20CFG/","title":"1.6 Adjusting CFG","text":"<p><pre><code>---  \nshare: true  \n---  \n</code></pre> CFG controls how much the model \"sticks\" to your prompt. </p> <p>Most of the time, you are fine just leaving it at 7. Some models perform better with lower CFG, some with higher CFG. I typically just open the CivitAI site of the model and look at the image prompts. If they are meaningfully higher orlower than 7, I adjust them into that direction, from 5 CFG to 11 CFG.</p> <p>If you are working with a model over a longer period of time, it can be worth experimenting what its preferred CFGs are. You may find yourself some prompts actually working better with a lower CFG, and some prompts warranting a higher CFG. </p> <p>This is the rough feeling I have from it, which I cannot really prove unfortunately:</p> <p>At higher CFG, it will repeat patterns. As an example, when prompting for fingers, it may prompt many fingers or fingernails.  At low CFG, it will try to rely more on the overall picture. As an example, when prompting for fingers, it will try to generate pictures where fingers are prominent. But because the structures are missing, the fingers often look melted or misshapen.  </p> <p>At super high CFG your image will get burnt. There are Extensionsto mitigate that. </p> <p>Next 1.7 Target Resolution</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.4-1.9%20Everything%20surrounding%20words/1.7%20Target%20Resolution/","title":"1.7 Target Resolution","text":"<p>The ideal output is 512x512. This is what most models were trained with. You can deviate from that, but some models cough Anythingv4.5 and all mixes with it in it cough don't like it at all. You still want to have at least one of the resolutions be a multiple of 64, and you also want to keep it beneath 1000px on both sides, as the model will start thinking there are TWO pictures it has to generate instead and thus generates fun stuff like two heads or two girls.</p> <p>Some models are explicitely trained on higher resolution, and model makers usually advertise it as a feature.</p> <p>Thanks to Winston Woof for calculating everything out!</p> Resolution Base 768 Base 512 Common Purpose 1:1 768 x 768 512 x 512 (Standard) 3:2 768 x 512 512 x 341 (Professional Photography) 4:3 768 x 576 512 x 384 (General Pictures) 16:9 768 x 432 512 x 288 (Wide Screen) 16:10 768 x 480 512 x 320 (Ultra Wide) <p>Colour aligns with what I would recommendmy recommendation</p> <p>Next: 1.8 Upscalers</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.4-1.9%20Everything%20surrounding%20words/1.8%20Upscalers/","title":"1.8 Upscalers","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>Upscaling for text2img:</p> <p>We want to mainly upscale this way because the model essentially adds details while upscaling. It will improve the details, especially for faces and hands. </p> <p>Here's how: </p> <p>1: Enable Hires. Fix. This way you use the upscaler. If you do not have this enabled, it won't upscale.</p> <p>2: The Latent Upscaler is not that recommended. Instead, the ESRGAN and ESRGAN anime Upscaler are recommended, as well as upscalers from the Model Database - Upscale Wiki. I was recommended Anifilm for anime, and Lollypop for 2.5D. I also heard good stuff about Remacri and UltraSharp. I cannot test because bad GPU, so this is community advice. If you do end up with the Latent Upscaler, set denoising to at least 0.5, possibly more. </p> <p>3: You can click on the greyed out boxes for an individual resolution. I recommend sticking to established resolutions, but if you need to set your own, make sure at least they are multiples of 64. Also, from what I heard, you can have the output resolution to 512x512 then upscale to a different aspect ratio. I still need to verify this myself.  If instead you are looking to upscale it by a factor, you can use the box left to this.</p> <p>4: Set denoising to 0.2 for smaller changes, 0.3 for a bit more bigger changes. If you are using Latent Upscaler, you need to use 0.5 or more, or it will look blurry. Denoising adds noise to the picture so that the model can use the noise and the original picture to increase the resolution. The higher the denoise is set, the more changes will happen.</p> <p>5: You can leave it at 0. 0 means that you do the same amount of steps as you had set for your sampler. If you have low VRAM, consider increasing the amount of steps, as then the VRAM spike at the start and at the end gets lower. </p> <p>Consider also using this Extension for less VRAM intensive Upscaling.</p> <p>Next: 2. Improving an existing picture</p>"},{"location":"2.%20Basic%20Prompting/1.%20Writing%20Prompts/1.4-1.9%20Everything%20surrounding%20words/x.x%20Commas%20%28ignore%2C%20outdated%2C%20will%20do%20more%20research%20way%20later%29/","title":"x.x Commas (ignore, outdated, will do more research way later)","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>I did more research, commas do seem to impact the image heavily. I will launch a few more researches later, but it seems like a topic to cover in the Intermediate Guide</p> <p>Outdated+maybe wrong for 1x+ series NVIDIA GPUs? I'm really getting confused by the GPUs: Commas do not majorly impact the image.  Most beginners think they do, but they don't. They slightly help with separation, and that's it. </p> <p>Okay, this warrants an explanation. Let's look at the following prompt:</p> <p>Need to generate all, bruh, I'll do that when I get better GPU</p> <p>I did a test on \"black, cat, girl\" or something like that and then on \"black cat girl\" and inbetween and the results did change but not majorly so, so in no result was there a cat, but might be samplesize low I recently read that it helps break tokens up since it actually can't consume sentences that long Needs more testing</p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.%20Improving%20an%20existing%20picture/","title":"2. Improving an existing picture","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>In chapter 1, we used Prompt Engineering to generate consistent quality pictures. Starting from 1.3 Token bleed and Token fighting, we dealt a bit more with how to increase the quality. </p> <p>In the process, you perhaps generated a hundred, maybe even a thousand or more pictures, and found one or two that you liked, were it not for some imperfections; a missing tag, an underrepresented tag, bad face, bad hands, artifacts, or some other minor detail in the picture.  </p> <p>Or maybe you did find a different picture elsewhere that you want to change. </p> <p>It makes no sense that we throw away a perfectly fine picture with slight imperfections. </p> <p>This is what this chapter is about \u2014 Making a singular picture better.</p> <p>Next: 2.1 Creating variations</p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.1%20Creating%20variations/","title":"2.1 Creating variations","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>Prompt refining is useful when you have a generated picture that you like, but want to use a light variation of it. This is similar to MidJourney's Variation option, with the key difference that you are in control of the variations. </p> <p>Firstly, we want to lock down the seed.  If you just generated the picture, then click this button to grab the seed:  Otherwise you will need to look at png info or at the file name, and manually enter the seed. </p> <p>Now, there are multiple options, depending on your wants and needs: Repair artifacts =&gt; Increase Sampler Steps or adjust negative prompt Respect / Disrespect prompt slightly more =&gt; Adjust CFG or adjust weight  Introduce slight variations in MidJourney style =&gt; Adjust prompt, or use the Variation (Intermediate Guide)</p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.1%20Creating%20variations/#repairing-artifacts","title":"Repairing artifacts","text":"<p>Artifacts typically happen as a result of low sample steps. Simply increasing it can help you here. If your sampler steps are already beyond 80, then reducing it down to 40 can also fix some artifacts with some models. </p> <p>Alternatively, you can add these tags to your negative prompt:</p> <p>Negative prompt:</p> <p>jpeg artifacts, aliasing, error, moire,  </p> <p>Furthermore these might be useful:</p> <p>Negative prompt:</p> <p>cropped, multi view, poorly drawn, anatomical nonsense, bad anatomy, bad feet, bad hands, bad proportions, bad quality, extra digits, extra limbs, missing digits, missing limbs</p> <p>If these methods are too much effort or change the picture too much, you can also look at 2.2 Img2Img to fix it. </p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.1%20Creating%20variations/#respect-disrespect-prompt-slightly-more","title":"Respect / Disrespect prompt slightly more","text":"<p>If it's only a few individual tags that need to be adjusted, you can try increasing/decreasing the weight of it. You can also try decreasing/increasing the other weights in the surrounding area, that are either not as important to you, or just are getting overrepresented. </p> <p>As an example, you might want red hair, but the red also bleeds into the eyes and the background, despite prompting e.g. blue eyes. In this case, lower the weight of red, and increase the weight of the blue.  red hair, blue eyes =&gt; (red:0.7) hair, (blue:1.15) eyes</p> <p>For CFG tuning, refer to 1.6 Adjusting CFG.</p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.1%20Creating%20variations/#introduce-slight-variations","title":"Introduce slight variations","text":"<p>Very slightly different:  Add random garbled words at the end, e.g. \"AHtalIth\" or \"dashioeth\". CLIP processes it and adds something random. You can adjust the prompt and weight. </p> <p>Slightly different:  Use the [[Variation Feature]]. </p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.1%20Creating%20variations/#it-doesnt-work","title":"It doesn't work.","text":"<p>If the above method doesn't work, it's about time we use different methods. </p> <p>Next: 2.2 Img2Img</p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.2%20Img2Img/","title":"2.2 Img2Img","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre> <p>Img2Img is great if you have a source image and want to generate something similar. How similar it is is something you control. </p> <p>Note:</p> <p>If you want the specific pose or composition, use ControlNet.  If you are using an Upscaler, I recommend this extension: https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111 If you want to change one region, e.g. to fix hands, refer to the next section, 2.3 Inpainting.</p> <p>Prompting an image to image picture is slightly different. It depends with the denoising strength, which is the most crucial parameter for img2img. </p> <p>The way img2img works is that it adds a bit of noise on top of the image, then removes the noise again. If the denoising is high, as in higher than 0.5, a lot of change will happen. If the denoising is low, not much will change. </p> <p>1 will cover the whole picture in noise, 0 will cover it in no noise. 0.3 is recommended for slight changes, 0.6 for bigger changes. You will need to play around with the denoising parameter for each individual picture. </p> <p>As for the sampler, use DPM++ 2M Karras.</p> <p>Let us look at a concrete example. </p> <p>I generated this picture of a lovely cat. </p> <p> But I want the cat to rest in the afternoon sun. </p> <p>So I throw it in Img2img, and just prompt as I usually would, without changing a thing.</p> <p></p> <p>The denoising is set to 0.75, and the sampling method is Euler A. </p> <p>I then used it again with DPM++ 2M Karras.   Even though it looks like a worse picture, it actually stuck more closely to the prompt. There is only one cat, and the eyes are now proper slits. </p> <p>Okay, I think that too many things are changing. The cat doesn't even have that cute blue ribbon anymore!</p> <p>So let's set denoising to a 0.5 first. You can see it circled red in the picture (I also highlighted the sampler selection from last step).   As you can see, it has the blue ribbon now. But honestly? Not much changed. Oh, yes, the left eye no longer looks that awkward, and the colour of the ribbon changed and some of the marks on the fur shifted around, but there is no grass. </p> <p>So let us shift denoising strength a little bit higher, to 0.6. </p> <p>The grass is here. But the blue tie/ribbon is gone. At this point, I think I will just add it to the prompt, and then set denoising a teensy tiny bit higher. It's already doing what it should, I just want a bit more of the outside (which I should also add to the prompt).</p> <p></p> <p>This kitten looks very marture all of sudden. I feel like the previous cute kitten is gone, even if more of the prompt is fulfilled. So, what do we do here?</p> <p>We have to set denoising lower, and hope we luck into a good seed that can bring us closer to outside. We can also increase the strength of grass and outside.   Welp, that didn't go well. Even though the cat remains the same, we just can't get the outside. Why is that so?</p> <p>The answer lies in how the model does things. Typically, when generating a picture, the model works with random noise at the start, and work its way through each step by removing a bit of the noise.  The noise has all sorts of colours, so that, even if the starting image does have quite a bit more red or blue, you can still remove red and blue, and have the green from the rest slowly spread through the picture.  This is not possible if there is no green at all, which is our starting picture. A way to fix this would be to just literally give the model more green to work with.  Shoddy paint net work: </p> <p>Result:  As you can see, it quickly it! You can even see the white stripe on the right, which previously made sense as the light source, but now makes no sense but has been left alone by the model. Why? Because the denoising is too low, and it didn't really know what to do with that space to begin with, so it just kept the white at each step. </p> <p>Okay, that solves the issue of when our prompt is not listened to. You can think of this as some advanced text prompting, where you also feed it a picture of your choosing at the start. The more you know about how the model handles pictures, the better you can handle this kind of prompting. A non-sensical picture says nothing to the model after all.</p> <p>But what if you are fine with the picture composition and everything in itself? All you want to do is fix these hands, darnit!</p> <p>Head no further than the next section, Inpainting, our savior and salvation.</p>"},{"location":"2.%20Basic%20Prompting/2.%20Improving%20Generated%20Output/2.3%20Inpainting/","title":"2.3 Inpainting","text":""},{"location":"2.%20Basic%20Prompting/3.%20Getting%20inspiration/3.1%20Civitai%20showcases/","title":"3.1 Civitai showcases","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre>"},{"location":"2.%20Basic%20Prompting/3.%20Getting%20inspiration/3.2%20Booru/","title":"3.2 Booru","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre>"},{"location":"2.%20Basic%20Prompting/4.%20Extras/4.1%20Textural%20Embeddings%20or%20Textural%20Inversions/","title":"4.1 Textural Embeddings or Textural Inversions","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre>"},{"location":"2.%20Basic%20Prompting/4.%20Extras/4.2%20LoRAs/","title":"4.2 LoRAs","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre>"},{"location":"2.%20Basic%20Prompting/4.%20Extras/4.3%20Extensions/","title":"4.3 Extensions","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre>"},{"location":"2.%20Basic%20Prompting/4.%20Extras/4.4%20Community/","title":"4.4 Community","text":"<pre><code>---  \nshare: true  \n---  \n</code></pre>"},{"location":"Misc/Extensions/","title":"Extensions","text":"<p>Extensions (Some I heard good things about: MultiDiffusion with Tiled VAE, Dynamic Thresholding, Prompt Gallery, Booru tag autocompletion)</p> <p></p> <p>You can find a long list here. It's not comprehensive, but it has quite a few of them. I quite recommend Booru Tag Autocomplete for anime models, as most anime models actually accept these tags (The tags work slightly differently, I will talk about that in Prompting).</p> <ol> <li>Extensions (Some I heard good things about: MultiDiffusion with Tiled VAE, Dynamic Thresholding, Prompt Gallery, Booru Tag Complete)</li> </ol>"},{"location":"Misc/Prompting%20for%20Dummies/","title":"Prompting for Dummies","text":"<p>TODO: Change to just make them set standard setting + standard negative</p> <ol> <li>Look around on Civit for a picture. You can start here.</li> <li>If present, press on the i in the bottom right corner of the picture.   If not present, repeat from Step 1, looking for a picture you haven't already looked at. </li> <li>Copy over the following data: Positive prompt, Negative Prompt, [[Sampler]], [[CFG]], </li> <li>Click on the Generate Button.</li> <li>Voila! You are done. You can also adjust the prompt to your needs.</li> </ol>"},{"location":"TODO/Colours/","title":"Colours","text":"<p>palered paleyellow palegreen</p> <p> INSERTLINKHERE INSERTLINKHERE</p>  Meow.  <p>\\</p>  Positive Prompt"},{"location":"TODO/Todo%20files/","title":"Todo files","text":"<p>1. Good vs Bad prompt results, 2. simple vs complex vs messy prompts, Novel AI or NAI, Stable Diffusion 1.5, Model List, general information list,  1. Fundamentals of Image Generation Models add pic to Tokens Extensions Add pics to 1. Installing Automatic1111 WebUI for the models Split Basic Prompting into 3. Write [[How to select a VAE| How to add VAE]] Write 1.2 Tags#^df4988 exceptions to the rule Write x.x Commas (ignore, outdated, will do more research way later) and do prompting  Figure out some OC for the purpose of this guide. Book? Girl? Cat? Do more research on negatives </p> <p>overview over webui</p>"}]}